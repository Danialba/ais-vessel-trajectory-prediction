{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "from lightgbm import LGBMRegressor\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "ais_train = pd.read_csv('/Users/danialbashir/Desktop/Projects/TDT4173-Main-Project-1/Data/ais_train.csv', delimiter='|')\n",
    "ais_test= pd.read_csv('/Users/danialbashir/Desktop/Projects/TDT4173-Main-Project-1/Data/ais_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Parameters\n",
    "# =========================\n",
    "\n",
    "number_of_days = 5  # 0 if validating\n",
    "remove_fraction = 0\n",
    "max_depth = 23\n",
    "n_estimators = 60\n",
    "mode = 'predict'  # or 'validate'\n",
    "remove_anomalies_speed = True  # Set to True to remove anomalies, False to keep all records\n",
    "MAX_REALISTIC_SPEED_KNOTS = 30  # Speed threshold for anomalies\n",
    "trim_train_data = True\n",
    "remove_cog_sog_anomalies=True\n",
    "\n",
    "number_of_lagged_coordinates = 1\n",
    "list_of_features = ['lat_diff', 'lon_diff', 'lag1_lat_diff', 'lag1_lon_diff']\n",
    "moving_avg_features = []\n",
    "\n",
    "# New parameter for rolling window size\n",
    "rolling_window_size = 5  # You can adjust this as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "Data split done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'])\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "\n",
    "print('Splitting data...')\n",
    "if mode == 'predict':\n",
    "    # Extract the target variables for the training set\n",
    "    X_val = ais_test.copy()\n",
    "    X_train = ais_train.copy()\n",
    "    #remove all vessels that are not in the neighboorhood of the test set \n",
    "    \n",
    "print('Data split done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training records: 1522065\n",
      "Interpolating records with speed_knots > 30...\n",
      "Interpolating out-of-range COG and SOG values...\n",
      "Anomalies interpolated. Remaining training records: 1522065\n"
     ]
    }
   ],
   "source": [
    "# Copy datasets\n",
    "ais_test = X_val.copy()\n",
    "ais_train = X_train.copy()\n",
    "\n",
    "# Ignore future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Ensure 'time' is in datetime format and sort data\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "ais_train.sort_values(['vesselId', 'time'], inplace=True)\n",
    "\n",
    "# Convert relevant columns to numeric types\n",
    "numeric_columns = ['latitude_vessel', 'longitude_vessel', 'cog', 'sog']\n",
    "ais_train[numeric_columns] = ais_train[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(f\"Initial training records: {len(ais_train)}\")\n",
    "\n",
    "# =========================\n",
    "# Anomaly Detection and Interpolation for Latitude/Longitude\n",
    "# =========================\n",
    "print(f\"Interpolating records with speed_knots > {MAX_REALISTIC_SPEED_KNOTS}...\")\n",
    "\n",
    "# Calculate shifted coordinates\n",
    "ais_train['prev_latitude'] = ais_train.groupby('vesselId')['latitude_vessel'].shift(1)\n",
    "ais_train['prev_longitude'] = ais_train.groupby('vesselId')['longitude_vessel'].shift(1)\n",
    "\n",
    "# Function to compute distance between two points\n",
    "def compute_distance(row):\n",
    "    if pd.isnull(row['prev_latitude']) or pd.isnull(row['prev_longitude']):\n",
    "        return np.nan\n",
    "    else:\n",
    "        point1 = (row['prev_latitude'], row['prev_longitude'])\n",
    "        point2 = (row['latitude_vessel'], row['longitude_vessel'])\n",
    "        return geodesic(point1, point2).meters\n",
    "\n",
    "# Calculate distance in meters between consecutive points\n",
    "ais_train['distance_meters'] = ais_train.apply(compute_distance, axis=1)\n",
    "\n",
    "# Calculate time difference in seconds within each 'vesselId'\n",
    "ais_train['time_diff_seconds'] = ais_train.groupby('vesselId')['time'].diff().dt.total_seconds()\n",
    "ais_train['time_diff_seconds'].replace(0, np.nan, inplace=True)  # Avoid division by zero\n",
    "\n",
    "# Calculate speed in meters per second and convert to knots\n",
    "ais_train['speed_m_s'] = ais_train['distance_meters'] / ais_train['time_diff_seconds']\n",
    "ais_train['speed_knots'] = ais_train['speed_m_s'] * 1.94384\n",
    "\n",
    "# Flag speed anomalies\n",
    "ais_train['speed_anomaly'] = ais_train['speed_knots'] > MAX_REALISTIC_SPEED_KNOTS\n",
    "\n",
    "# Set anomalies to NaN in 'latitude_vessel' and 'longitude_vessel'\n",
    "ais_train.loc[ais_train['speed_anomaly'], ['latitude_vessel', 'longitude_vessel']] = np.nan\n",
    "\n",
    "# =========================\n",
    "# Anomaly Detection and Interpolation for COG/SOG\n",
    "# =========================\n",
    "print(\"Interpolating out-of-range COG and SOG values...\")\n",
    "\n",
    "# Flag COG anomalies\n",
    "ais_train['cog_anomaly'] = (ais_train['cog'] >= 360) | (ais_train['cog'] < 0)\n",
    "ais_train['sog_anomaly'] = ais_train['sog'] > 30\n",
    "\n",
    "# Set out-of-range COG and SOG values to NaN for interpolation\n",
    "ais_train.loc[ais_train['cog_anomaly'], 'cog'] = np.nan\n",
    "ais_train.loc[ais_train['sog_anomaly'], 'sog'] = np.nan\n",
    "\n",
    "# =========================\n",
    "# Interpolation for All Features\n",
    "# =========================\n",
    "# Set 'time' as the index for interpolation\n",
    "ais_train.set_index('time', inplace=True)\n",
    "\n",
    "# Interpolate within each 'vesselId' group and fill missing values forward/backward\n",
    "ais_train[['latitude_vessel', 'longitude_vessel', 'cog', 'sog']] = ais_train.groupby('vesselId')[\n",
    "    ['latitude_vessel', 'longitude_vessel', 'cog', 'sog']\n",
    "].transform(lambda group: group.interpolate(method='time').ffill().bfill())\n",
    "\n",
    "# Reset the index after interpolation\n",
    "ais_train.reset_index(inplace=True)\n",
    "\n",
    "# Drop the anomaly flag columns as they're no longer needed\n",
    "ais_train.drop(columns=['speed_anomaly', 'cog_anomaly', 'sog_anomaly'], inplace=True)\n",
    "\n",
    "# Drop any remaining NaN values resulting from shifts or interpolation limits\n",
    "ais_train.dropna(subset=['latitude_vessel', 'longitude_vessel', 'cog', 'sog'], inplace=True)\n",
    "\n",
    "print(f\"Anomalies interpolated. Remaining training records: {len(ais_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_distance\n",
      "1    666176\n",
      "0    660716\n",
      "2    193798\n",
      "Name: count, dtype: int64\n",
      "cluster_distance\n",
      "0    159\n",
      "1    378\n",
      "2    150\n",
      "Name: vesselId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# Ensure the DataFrames are sorted by 'vesselId' and 'time'\n",
    "ais_train = ais_train.sort_values(['vesselId', 'time'] )\n",
    "ais_test = ais_test.sort_values(['vesselId', 'time'])\n",
    "\n",
    "# 3. Calculate the time difference between consecutive rows within each 'vesselId'\n",
    "ais_train['time_diff_seconds'] = ais_train.groupby('vesselId')['time'].diff().dt.total_seconds()\n",
    "\n",
    "\n",
    "# 4. Create lagged time difference features\n",
    "for i in range(1, number_of_lagged_coordinates + 2):\n",
    "    if i == 1:\n",
    "        # For lag 1, use the current 'time_diff_seconds' without shifting\n",
    "        ais_train[f'prev{i}_time_diff_seconds'] = ais_train['time_diff_seconds']\n",
    "        ais_test[f'prev{i}_time_diff_seconds'] = np.nan\n",
    "    else:\n",
    "        # For lags >1, shift 'time_diff_seconds' by (i -1)\n",
    "        ais_train[f'prev{i}_time_diff_seconds'] = ais_train.groupby('vesselId')['time_diff_seconds'].shift(i - 1)\n",
    "        ais_test[f'prev{i}_time_diff_seconds'] = np.nan\n",
    "    \n",
    "    # Append the feature name to the list of features\n",
    "for i in range(1, number_of_lagged_coordinates + 1):\n",
    "    list_of_features.append(f'prev{i}_time_diff_seconds')\n",
    "\n",
    "\n",
    "# 5. (Optional) Drop the intermediate 'time_diff_seconds' column if not needed\n",
    "ais_train.drop(columns=['time_diff_seconds'], inplace=True)\n",
    "\n",
    "\n",
    "#generate number_of_lagged_coordinates lagged coordinates and corresponting time differences\n",
    "for i in range(1, number_of_lagged_coordinates + 2):\n",
    "    ais_train[f'prev{i}_longitude'] = ais_train.groupby('vesselId')['longitude_vessel'].shift(i)\n",
    "    ais_train[f'prev{i}_latitude'] = ais_train.groupby('vesselId')['latitude_vessel'].shift(i)\n",
    "    ais_test[f'prev{i}_longitude'] = np.nan\n",
    "    ais_test[f'prev{i}_latitude'] = np.nan\n",
    "for i in range (1, number_of_lagged_coordinates + 1):\n",
    "    list_of_features.extend([f'prev{i}_longitude', f'prev{i}_latitude'])\n",
    "    \n",
    "\n",
    "        \n",
    "if moving_avg_features:\n",
    "    for feature in moving_avg_features.copy():\n",
    "        if feature=='cog':\n",
    "            #decompose the cog feature\n",
    "            ais_train['xcog'] = ais_train['cog'].apply(lambda x: np.cos(np.radians(x)))\n",
    "            ais_train['ycog'] = ais_train['cog'].apply(lambda x: np.sin(np.radians(x)))\n",
    "            ais_train['xcog_moving_avg'] = ais_train.groupby('vesselId')['xcog'].transform(lambda x: x.rolling(rolling_window_size, min_periods=1).mean())\n",
    "            ais_train['ycog_moving_avg'] = ais_train.groupby('vesselId')['ycog'].transform(lambda x: x.rolling(rolling_window_size, min_periods=1).mean())\n",
    "            ais_test['xcog_moving_avg'] = np.nan\n",
    "            ais_test['ycog_moving_avg'] = np.nan\n",
    "            moving_avg_features.remove('cog')\n",
    "            moving_avg_features.append('xcog')\n",
    "            moving_avg_features.append('ycog')\n",
    "            list_of_features.append('xcog_moving_avg')\n",
    "            list_of_features.append('ycog_moving_avg')\n",
    "        else:\n",
    "            ais_train[f'{feature}_moving_avg'] = ais_train.groupby('vesselId')[feature].transform(lambda x: x.rolling(rolling_window_size, min_periods=1).mean())\n",
    "            ais_test[f'{feature}_moving_avg'] = np.nan\n",
    "            list_of_features.append(f'{feature}_moving_avg')\n",
    "\n",
    "#drop rows with NaN values due to shift\n",
    "ais_train.dropna(subset=['prev1_longitude', 'prev1_latitude', 'prev2_longitude', 'prev2_latitude'], inplace=True)\n",
    "            \n",
    "for feature in list_of_features.copy():\n",
    "    if feature == 'weekday':      \n",
    "        ais_train['weekday'] = ais_train['time'].dt.weekday.astype(int)\n",
    "        ais_test['weekday'] = ais_test['time'].dt.weekday.astype(int)\n",
    "    elif feature == 'month':\n",
    "        ais_train['month'] = ais_train['time'].dt.month\n",
    "        ais_test['month'] = ais_test['time'].dt.month\n",
    "    elif feature == 'hour':\n",
    "        ais_train['hour'] = ais_train['time'].dt.hour\n",
    "        ais_test['hour'] = ais_test['time'].dt.hour\n",
    "    # seconds after 1970\n",
    "    elif feature == 'total_seconds':\n",
    "        # Calculate `total_seconds` in a consistent manner across training and test sets\n",
    "        reference_time = min(ais_train['time'].min(), ais_test['time'].min())\n",
    "        # Update `total_seconds` calculation\n",
    "        ais_train['total_seconds'] = (ais_train['time'] - reference_time).dt.total_seconds()\n",
    "        ais_test['total_seconds'] = (ais_test['time'] - reference_time).dt.total_seconds()\n",
    "    elif feature == 'day_of_year':\n",
    "        ais_train['day_of_year'] = ais_train['time'].dt.dayofyear\n",
    "        ais_test['day_of_year'] = ais_test['time'].dt.dayofyear\n",
    "    elif feature == 'lon_diff':\n",
    "        ais_train['lon_diff'] = ((ais_train['prev1_longitude'] - ais_train['prev2_longitude'])/ais_train['prev2_time_diff_seconds']*ais_train['prev1_time_diff_seconds'])\n",
    "        ais_test['lon_diff'] = np.nan\n",
    "    elif feature == 'lat_diff':\n",
    "        ais_train['lat_diff'] = (((ais_train['prev1_latitude'] - ais_train['prev2_latitude'])/ais_train['prev2_time_diff_seconds'])*ais_train['prev1_time_diff_seconds'])\n",
    "        ais_test['lat_diff'] = np.nan\n",
    "    elif feature == 'distance':\n",
    "        ais_train['distance'] = ais_train.apply(\n",
    "        lambda x: geodesic(\n",
    "            (x['prev2_latitude'], x['prev2_longitude']), \n",
    "            (x['prev1_latitude'], x['prev1_longitude'])  # Corrected\n",
    "            ).meters, \n",
    "            axis=1)\n",
    "        ais_test['distance'] = np.nan  # As per your original logic\n",
    "    \n",
    "    elif feature.startswith('lag') and ('lat_diff' in feature or 'lon_diff' in feature):\n",
    "        lag_num = int(feature.split('_')[0][3:])\n",
    "        diff_type = 'lat_diff' if 'lat_diff' in feature else 'lon_diff'\n",
    "        ais_train[feature] = ais_train.groupby('vesselId')[diff_type].shift(lag_num)\n",
    "        ais_test[feature] = ais_test.groupby('vesselId')[diff_type].shift(lag_num)\n",
    "    \n",
    "ais_train['haversine_distance'] = 6371000 * 2 * np.arcsin(\n",
    "np.sqrt(\n",
    "np.sin(np.radians((ais_train['latitude_vessel'] - ais_train['prev1_latitude']) / 2))**2 +\n",
    "np.cos(np.radians(ais_train['prev1_latitude'])) * np.cos(np.radians(ais_train['latitude_vessel'])) *\n",
    "np.sin(np.radians((ais_train['longitude_vessel'] - ais_train['prev1_longitude']) / 2))**2\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define bins and labels in meters\n",
    "bins = [-np.inf, 0.375e7, 1.1e7, np.inf]  # <3.75e6, 3.75e6-11e6, >11e6 meters. Found through EDA.\n",
    "labels = ['cluster_distance_0', 'cluster_distance_1', 'cluster_distance_2']\n",
    "\n",
    "# Calculate average Haversine distance per vessel\n",
    "max_haversine = ais_train.groupby('vesselId')['haversine_distance'].max()\n",
    "\n",
    "# After assigning cluster labels\n",
    "cluster_labels = pd.cut(max_haversine, bins=bins, labels=[0, 1, 2])\n",
    "\n",
    "# Create a mapping from 'vesselId' to cluster number\n",
    "cluster_mapping = cluster_labels.to_dict()\n",
    "\n",
    "#Map cluster labels to ais_train and ais_test\n",
    "ais_train['cluster_distance'] = ais_train['vesselId'].map(cluster_mapping)\n",
    "ais_test['cluster_distance'] = ais_test['vesselId'].map(cluster_mapping)\n",
    "\n",
    "                \n",
    "# Create a dictionary mapping vesselId to cluster number\n",
    "#cluster_dict = ais_train.set_index('vesselId')[['cluster_distance_0', 'cluster_distance_1', 'cluster_distance_2']].idxmax(axis=1).str[-1].astype(int).to_dict()\n",
    "\n",
    "#print number of vessels in each cluster\n",
    "print(ais_train['cluster_distance'].value_counts())\n",
    "#print number of unique vessels in each cluster\n",
    "print(ais_train.groupby('cluster_distance')['vesselId'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ais_train.dropna(subset=list_of_features, inplace=True)\n",
    "\n",
    "#sort by vesselId and time\n",
    "ais_train.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "ais_test.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1707.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if trim_train_data:\n",
    "    # Find the median number of rows per vessel in the training set\n",
    "    median_rows_per_vessel = ais_train.groupby('vesselId').size().median()\n",
    "    print(median_rows_per_vessel)\n",
    "\n",
    "    # Calculate the number of rows per vessel in the training set\n",
    "    rows_per_vessel = ais_train.groupby('vesselId').size()\n",
    "\n",
    "    # Find vessels with fewer than the median number of rows\n",
    "    vessels_with_few_rows = set(rows_per_vessel[rows_per_vessel < 100].index)\n",
    "\n",
    "    # Find vessels that are in the training set but not in the test set\n",
    "    vessels_not_in_test = set(ais_train['vesselId']) - set(ais_test['vesselId'])\n",
    "\n",
    "    # Combine conditions: vessels with few rows AND not in test set\n",
    "    vessels_to_remove = vessels_with_few_rows & vessels_not_in_test\n",
    "\n",
    "    # Remove rows corresponding to these vessels from the training set\n",
    "    ais_train = ais_train[~ais_train['vesselId'].isin(vessels_to_remove)]\n",
    "\n",
    "    #print number of negative time differences\n",
    "    print(ais_train[ais_train['prev1_time_diff_seconds'] < 0].shape[0])\n",
    "    \n",
    "if mode == 'validate':\n",
    "    #ais_test = ais_test[~ais_test['vesselId'].isin(vessels_to_remove)]\n",
    "    #if boat is not in train set but in test set, remove it from test set\n",
    "    ais_test = ais_test[ais_test['vesselId'].isin(ais_train['vesselId'])]\n",
    "    \n",
    "\n",
    "\n",
    "moving_avg_dicts = {}\n",
    "\n",
    "for feature in moving_avg_features:\n",
    "    moving_avg_dicts[f\"{feature}_moving_avg\"] = {}\n",
    "    \n",
    "    \n",
    "# Initialize with training data\n",
    "for vessel_id, group in ais_train.groupby('vesselId'):\n",
    "    #sort by time\n",
    "    group.sort_values(by='time', inplace=True) \n",
    "    for feature in moving_avg_features:\n",
    "        moving_avg_dicts[f\"{feature}_moving_avg\"][vessel_id] = deque(group[f'{feature}_moving_avg'].tail(rolling_window_size), maxlen=rolling_window_size)\n",
    "        \n",
    "        \n",
    "def update_moving_averages(vessel_id, feature, new_value):\n",
    "    moving_avg_dicts[f\"{feature}_moving_avg\"][vessel_id].append(new_value)\n",
    "    return np.mean(moving_avg_dicts[f\"{feature}_moving_avg\"][vessel_id])   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "Processing cluster 0...\n",
      "Training longitude model for cluster 0...\n",
      "Training latitude model for cluster 0...\n",
      "Feature importance for longitude model (Cluster 0):\n",
      "lat_diff: 0.0000\n",
      "lon_diff: 0.0002\n",
      "lag1_lat_diff: 0.0000\n",
      "lag1_lon_diff: 0.0000\n",
      "prev1_time_diff_seconds: 0.0000\n",
      "prev1_longitude: 0.9965\n",
      "prev1_latitude: 0.0032\n",
      "Feature importance for latitude model (Cluster 0):\n",
      "lat_diff: 0.0006\n",
      "lon_diff: 0.0003\n",
      "lag1_lat_diff: 0.0000\n",
      "lag1_lon_diff: 0.0000\n",
      "prev1_time_diff_seconds: 0.0003\n",
      "prev1_longitude: 0.0002\n",
      "prev1_latitude: 0.9986\n",
      "Processing cluster 1...\n",
      "Training longitude model for cluster 1...\n",
      "Training latitude model for cluster 1...\n",
      "Feature importance for longitude model (Cluster 1):\n",
      "lat_diff: 0.0002\n",
      "lon_diff: 0.0020\n",
      "lag1_lat_diff: 0.0001\n",
      "lag1_lon_diff: 0.0000\n",
      "prev1_time_diff_seconds: 0.0018\n",
      "prev1_longitude: 0.9957\n",
      "prev1_latitude: 0.0002\n",
      "Feature importance for latitude model (Cluster 1):\n",
      "lat_diff: 0.0013\n",
      "lon_diff: 0.0004\n",
      "lag1_lat_diff: 0.0001\n",
      "lag1_lon_diff: 0.0001\n",
      "prev1_time_diff_seconds: 0.0015\n",
      "prev1_longitude: 0.0003\n",
      "prev1_latitude: 0.9963\n",
      "Processing cluster 2...\n",
      "Training longitude model for cluster 2...\n",
      "Training latitude model for cluster 2...\n",
      "Feature importance for longitude model (Cluster 2):\n",
      "lat_diff: 0.0002\n",
      "lon_diff: 0.0016\n",
      "lag1_lat_diff: 0.0001\n",
      "lag1_lon_diff: 0.0001\n",
      "prev1_time_diff_seconds: 0.0031\n",
      "prev1_longitude: 0.9948\n",
      "prev1_latitude: 0.0002\n",
      "Feature importance for latitude model (Cluster 2):\n",
      "lat_diff: 0.0014\n",
      "lon_diff: 0.0005\n",
      "lag1_lat_diff: 0.0001\n",
      "lag1_lon_diff: 0.0001\n",
      "prev1_time_diff_seconds: 0.0019\n",
      "prev1_longitude: 0.0003\n",
      "prev1_latitude: 0.9957\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "print('Training models...')\n",
    "\n",
    "# Initialize dictionaries to store models for each cluster\n",
    "models_long = {}\n",
    "models_lat = {}\n",
    "\n",
    "# Train separate longitude and latitude models for each cluster\n",
    "for cluster in [0, 1, 2]:\n",
    "    print(f'Processing cluster {cluster}...')\n",
    "    if cluster == 2:\n",
    "        # For cluster 2, combine data from clusters 0, 1, and 2\n",
    "        X_cluster = ais_train[ais_train['cluster_distance'].isin([0, 1, 2])][list_of_features]\n",
    "        y_long_cluster = ais_train[ais_train['cluster_distance'].isin([0, 1, 2])]['longitude_vessel']\n",
    "        y_lat_cluster = ais_train[ais_train['cluster_distance'].isin([0, 1, 2])]['latitude_vessel']\n",
    "    elif cluster == 1:\n",
    "        # For cluster 1, combine data from clusters 0 and 1\n",
    "        X_cluster = ais_train[ais_train['cluster_distance'].isin([0, 1])][list_of_features]\n",
    "        y_long_cluster = ais_train[ais_train['cluster_distance'].isin([0, 1])]['longitude_vessel']\n",
    "        y_lat_cluster = ais_train[ais_train['cluster_distance'].isin([0, 1])]['latitude_vessel']\n",
    "    else:\n",
    "        X_cluster = ais_train[ais_train['cluster_distance'] == cluster][list_of_features]\n",
    "        y_long_cluster = ais_train[ais_train['cluster_distance'] == cluster]['longitude_vessel']\n",
    "        y_lat_cluster = ais_train[ais_train['cluster_distance'] == cluster]['latitude_vessel']\n",
    "\n",
    "    if not X_cluster.empty:\n",
    "        print(f'Training longitude model for cluster {cluster}...')\n",
    "        models_long[cluster] = RandomForestRegressor(\n",
    "            n_estimators=n_estimators, \n",
    "            max_depth=max_depth, \n",
    "            random_state=42, \n",
    "            n_jobs=-1, \n",
    "            verbose=0\n",
    "        ).fit(X_cluster, y_long_cluster)\n",
    "        \n",
    "        print(f'Training latitude model for cluster {cluster}...')\n",
    "        models_lat[cluster] = RandomForestRegressor(\n",
    "            n_estimators=n_estimators, \n",
    "            max_depth=max_depth, \n",
    "            random_state=42, \n",
    "            n_jobs=-1, \n",
    "            verbose=0\n",
    "        ).fit(X_cluster, y_lat_cluster)\n",
    "        \n",
    "        # Print feature importance\n",
    "        print(f'Feature importance for longitude model (Cluster {cluster}):')\n",
    "        for feature, importance in zip(list_of_features, models_long[cluster].feature_importances_):\n",
    "            print(f'{feature}: {importance:.4f}')\n",
    "        \n",
    "        print(f'Feature importance for latitude model (Cluster {cluster}):')\n",
    "        for feature, importance in zip(list_of_features, models_lat[cluster].feature_importances_):\n",
    "            print(f'{feature}: {importance:.4f}')\n",
    "    else:\n",
    "        print(f\"No data for cluster {cluster}. Skipping model training for this cluster.\")\n",
    "\n",
    "print('Model training complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0% complete.\n",
      "Processing: 1% complete.\n",
      "Processing: 2% complete.\n",
      "Processing: 3% complete.\n",
      "Processing: 4% complete.\n",
      "Processing: 5% complete.\n",
      "Processing: 6% complete.\n",
      "Processing: 7% complete.\n",
      "Processing: 8% complete.\n",
      "Processing: 9% complete.\n",
      "Processing: 10% complete.\n",
      "Processing: 11% complete.\n",
      "Processing: 12% complete.\n",
      "Processing: 13% complete.\n",
      "Processing: 14% complete.\n",
      "Processing: 15% complete.\n",
      "Processing: 16% complete.\n",
      "Processing: 17% complete.\n",
      "Processing: 18% complete.\n",
      "Processing: 19% complete.\n",
      "Processing: 20% complete.\n",
      "Processing: 21% complete.\n",
      "Processing: 22% complete.\n",
      "Processing: 23% complete.\n",
      "Processing: 24% complete.\n",
      "Processing: 25% complete.\n",
      "Processing: 26% complete.\n",
      "Processing: 27% complete.\n",
      "Processing: 28% complete.\n",
      "Processing: 29% complete.\n",
      "Processing: 30% complete.\n",
      "Processing: 31% complete.\n",
      "Processing: 32% complete.\n",
      "Processing: 33% complete.\n",
      "Processing: 34% complete.\n",
      "Processing: 35% complete.\n",
      "Processing: 36% complete.\n",
      "Processing: 37% complete.\n",
      "Processing: 38% complete.\n",
      "Processing: 39% complete.\n",
      "Processing: 40% complete.\n",
      "Processing: 41% complete.\n",
      "Processing: 42% complete.\n",
      "Processing: 43% complete.\n",
      "Processing: 44% complete.\n",
      "Processing: 45% complete.\n",
      "Processing: 46% complete.\n",
      "Processing: 47% complete.\n",
      "Processing: 48% complete.\n",
      "Processing: 49% complete.\n",
      "Processing: 50% complete.\n",
      "Processing: 51% complete.\n",
      "Processing: 52% complete.\n",
      "Processing: 53% complete.\n",
      "Processing: 54% complete.\n",
      "Processing: 55% complete.\n",
      "Processing: 56% complete.\n",
      "Processing: 57% complete.\n",
      "Processing: 58% complete.\n",
      "Processing: 59% complete.\n",
      "Processing: 60% complete.\n",
      "Processing: 61% complete.\n",
      "Processing: 62% complete.\n",
      "Processing: 63% complete.\n",
      "Processing: 64% complete.\n",
      "Processing: 65% complete.\n",
      "Processing: 66% complete.\n",
      "Processing: 67% complete.\n",
      "Processing: 68% complete.\n",
      "Processing: 69% complete.\n",
      "Processing: 70% complete.\n",
      "Processing: 71% complete.\n",
      "Processing: 72% complete.\n",
      "Processing: 73% complete.\n",
      "Processing: 74% complete.\n",
      "Processing: 75% complete.\n",
      "Processing: 76% complete.\n",
      "Processing: 77% complete.\n",
      "Processing: 78% complete.\n",
      "Processing: 79% complete.\n",
      "Processing: 80% complete.\n",
      "Processing: 81% complete.\n",
      "Processing: 82% complete.\n",
      "Processing: 83% complete.\n",
      "Processing: 84% complete.\n",
      "Processing: 85% complete.\n",
      "Processing: 86% complete.\n",
      "Processing: 87% complete.\n",
      "Processing: 88% complete.\n",
      "Processing: 89% complete.\n",
      "Processing: 90% complete.\n",
      "Processing: 91% complete.\n",
      "Processing: 92% complete.\n",
      "Processing: 93% complete.\n",
      "Processing: 94% complete.\n",
      "Processing: 95% complete.\n",
      "Processing: 96% complete.\n",
      "Processing: 97% complete.\n",
      "Processing: 98% complete.\n",
      "Processing: 99% complete.\n",
      "Processing: 100% complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the prediction set from ais_test\n",
    "prediction_data = ais_test.copy()\n",
    "prediction_data['predicted_longitude'] = np.nan\n",
    "prediction_data['predicted_latitude'] = np.nan\n",
    "\n",
    "# Set up groups for efficient lookup by vessel\n",
    "train_set_sorted = ais_train.sort_values(by=['vesselId', 'time'])\n",
    "ais_train_groups = train_set_sorted.groupby('vesselId')\n",
    "\n",
    "# Initialize counters for progress tracking\n",
    "total_rows = len(prediction_data)\n",
    "progress_interval = max(total_rows // 100, 1)  # Calculate for 1% intervals, minimum 1\n",
    "processed_rows = 0  # Initialize counter for processed rows\n",
    "\n",
    "# Set model to use a single thread for faster individual row processing\n",
    "for model in models_long.values():\n",
    "    model.set_params(n_jobs=1)\n",
    "for model in models_lat.values():\n",
    "    model.set_params(n_jobs=1)\n",
    "\n",
    "# Loop through each vessel to perform sequential predictions\n",
    "for vessel_id, group in prediction_data.groupby('vesselId'):\n",
    "    # Get cluster number for the vessel\n",
    "    cluster_num = cluster_mapping.get(vessel_id)\n",
    "    \n",
    "    # Get past data for this vessel\n",
    "    past_data = ais_train_groups.get_group(vessel_id)\n",
    "    # Track last known position within each vessel group\n",
    "    last_known_data = past_data.iloc[-1].copy()\n",
    "    \n",
    "    last_known_data['predicted_longitude'] = last_known_data['longitude_vessel']\n",
    "    last_known_data['predicted_latitude'] = last_known_data['latitude_vessel']\n",
    "    \n",
    "    # Iterate over each row for this vessel in prediction_data\n",
    "    for idx, (index, row) in enumerate(group.iterrows()):\n",
    "        dict_avgs = {}\n",
    "        \n",
    "        for feature in moving_avg_features:\n",
    "            if feature == 'latitude_vessel':\n",
    "                dict_avgs[feature] = update_moving_averages(\n",
    "                    vessel_id,\n",
    "                    feature,\n",
    "                    last_known_data['predicted_latitude']\n",
    "                )\n",
    "            elif feature == 'longitude_vessel':\n",
    "                dict_avgs[feature] = update_moving_averages(\n",
    "                    vessel_id,\n",
    "                    feature,\n",
    "                    last_known_data['predicted_longitude']\n",
    "                )\n",
    "            else:\n",
    "                dict_avgs[feature] = update_moving_averages(\n",
    "                    vessel_id,\n",
    "                    feature,\n",
    "                    last_known_data[f'{feature}_moving_avg']\n",
    "                )\n",
    "        \n",
    "        # Assign moving averages to the current row\n",
    "        for feature in moving_avg_features:\n",
    "            prediction_data.at[index, f'{feature}_moving_avg'] = dict_avgs[feature]\n",
    "        \n",
    "        # Update lagged time difference dynamically\n",
    "        time_diff = (row['time'] - last_known_data['time']).total_seconds()\n",
    "        \n",
    "        prediction_data.at[index, 'prev1_time_diff_seconds'] = time_diff\n",
    "        prediction_data.at[index, 'prev2_time_diff_seconds'] = last_known_data['prev1_time_diff_seconds']\n",
    "        \n",
    "        # Dynamically update lagged coordinates\n",
    "        prediction_data.at[index, 'prev1_longitude'] = last_known_data['predicted_longitude']\n",
    "        prediction_data.at[index, 'prev1_latitude'] = last_known_data['predicted_latitude']\n",
    "        prediction_data.at[index, 'prev2_latitude'] = last_known_data['prev1_latitude']\n",
    "        prediction_data.at[index, 'prev2_longitude'] = last_known_data['prev1_longitude']\n",
    "        \n",
    "        if 'lat_diff' in list_of_features:\n",
    "            prediction_data.at[index, 'lat_diff'] = (((last_known_data['predicted_latitude'] - last_known_data['prev1_latitude']) / last_known_data['prev1_time_diff_seconds']) * time_diff)\n",
    "            prediction_data.at[index, 'lon_diff'] = (((last_known_data['predicted_longitude'] - last_known_data['prev1_longitude']) / last_known_data['prev1_time_diff_seconds']) * time_diff)\n",
    "        \n",
    "        if 'distance' in list_of_features:\n",
    "            prediction_data.at[index, 'distance'] = geodesic(\n",
    "                (last_known_data['prev1_latitude'], last_known_data['prev1_longitude']),\n",
    "                (last_known_data['predicted_latitude'], last_known_data['predicted_longitude'])\n",
    "            ).meters\n",
    "        \n",
    "        if 'xcogxtime' in list_of_features:\n",
    "            prediction_data.at[index, 'xcogxtime'] = prediction_data.at[index, 'xcog_moving_avg'] * time_diff\n",
    "                \n",
    "        if 'ycogxtime' in list_of_features:\n",
    "            prediction_data.at[index, 'ycogxtime'] = prediction_data.at[index, 'ycog_moving_avg'] * time_diff\n",
    "        \n",
    "        if 'sogxtime' in list_of_features:\n",
    "            prediction_data.at[index, 'sogxtime'] = prediction_data.at[index, 'sog_moving_avg'] * time_diff\n",
    "        \n",
    "        # Update multiple lag features\n",
    "        for feature in list_of_features:\n",
    "            if feature.startswith('lag') and ('lat_diff' in feature or 'lon_diff' in feature):\n",
    "                lag_num = int(feature.split('_')[0][3:])\n",
    "                diff_type = 'lat_diff' if 'lat_diff' in feature else 'lon_diff'\n",
    "                if lag_num == 1:\n",
    "                    prediction_data.at[index, feature] = last_known_data[diff_type]\n",
    "                else:\n",
    "                    prev_lag_feature = f'lag{lag_num - 1}_{diff_type}'\n",
    "                    prediction_data.at[index, feature] = last_known_data.get(prev_lag_feature, np.nan)\n",
    "\n",
    "\n",
    "        if number_of_lagged_coordinates > 1:\n",
    "            # Update higher-order lag features based on latest predictions\n",
    "            for i in range(2, number_of_lagged_coordinates + 1):  \n",
    "                # Shift previous predictions back for each lag\n",
    "                prediction_data.at[index, f'prev{i}_longitude'] = last_known_data.get(f'prev{i-1}_longitude', np.nan)\n",
    "                prediction_data.at[index, f'prev{i}_latitude'] = last_known_data.get(f'prev{i-1}_latitude', np.nan)\n",
    "                prediction_data.at[index, f'prev{i}_time_diff_seconds'] = last_known_data.get(f'prev{i-1}_time_diff_seconds', np.nan)\n",
    "        \n",
    "        features = prediction_data.loc[index, list_of_features].to_frame().T\n",
    "        \n",
    "        if features.isnull().any().any():\n",
    "            print(f\"Invalid features at index {index}. Skipping prediction.\")\n",
    "            continue\n",
    "                \n",
    "        # Select the appropriate models based on cluster\n",
    "        model_long_cluster = models_long.get(cluster_num)\n",
    "        model_lat_cluster = models_lat.get(cluster_num)\n",
    "        \n",
    "        # Make predictions for the current row\n",
    "        pred_long = model_long_cluster.predict(features)[0]\n",
    "        pred_lat = model_lat_cluster.predict(features)[0]\n",
    "        \n",
    "        # Store predictions\n",
    "        prediction_data.at[index, 'predicted_longitude'] = pred_long\n",
    "        prediction_data.at[index, 'predicted_latitude'] = pred_lat\n",
    "        \n",
    "        # Update last known data with current prediction\n",
    "        last_known_data = prediction_data.loc[index].copy()\n",
    "        last_known_data['time'] = row['time']\n",
    "        \n",
    "         # Handling lag_lat_diff and lag_lon_diff\n",
    "        for feature in list_of_features:\n",
    "            if feature.startswith('lag') and 'lat_diff' in feature:\n",
    "                prediction_data.at[index, feature] = last_known_data['lat_diff']\n",
    "            if feature.startswith('lag') and 'lon_diff' in feature:\n",
    "                prediction_data.at[index, feature] = last_known_data['lon_diff']\n",
    "        \n",
    "        # Increment the counter for processed rows and print progress\n",
    "        processed_rows += 1\n",
    "        if processed_rows % progress_interval == 0:\n",
    "            percent_complete = int((processed_rows / total_rows) * 100)\n",
    "            print(f\"Processing: {percent_complete}% complete.\")\n",
    "                \n",
    "print(\"Processing: 100% complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed predictions saved to Detailed_Predictions.csv.\n",
      "Submission predictions saved to Submission.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#outputs\n",
    "if mode=='predict':\n",
    "    \n",
    "    prediction_data['longitude_predicted'] = prediction_data['predicted_longitude']\n",
    "    prediction_data['latitude_predicted'] = prediction_data['predicted_latitude']\n",
    "    detailed_predictions = prediction_data\n",
    "    \n",
    "    detailed_predictions.to_csv('Detailed_Predictions.csv', index=False)\n",
    "    print(\"Detailed predictions saved to Detailed_Predictions.csv.\")\n",
    "\n",
    "    submission_predictions = prediction_data[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "    submission_predictions.to_csv('Submission.csv', index=False)\n",
    "    print(\"Submission predictions saved to Submission.csv.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tdt4173-main-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

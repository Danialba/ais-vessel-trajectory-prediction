{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "from lightgbm import LGBMRegressor\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "ais_train = pd.read_csv('/Users/danialbashir/Desktop/Projects/TDT4173-Main-Project-1/Data/ais_train.csv', delimiter='|')\n",
    "ais_test= pd.read_csv('/Users/danialbashir/Desktop/Projects/TDT4173-Main-Project-1/Data/ais_test.csv')\n",
    "vessels = pd.read_csv('/Users/danialbashir/Desktop/Projects/TDT4173-Main-Project-1/Data/vessels.csv', delimiter='|')\n",
    "ports = pd.read_csv('/Users/danialbashir/Desktop/Projects/TDT4173-Main-Project-1/Data/ports.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Parameters\n",
    "# =========================\n",
    "\n",
    "number_of_days = 5  # 0 if not just predicting\n",
    "remove_fraction = 0\n",
    "max_depth = 23\n",
    "n_estimators = 50\n",
    "mode = 'predict'  # or 'validate'\n",
    "remove_anomalies_speed = True  # Set to True to remove anomalies, False to keep all records\n",
    "MAX_REALISTIC_SPEED_KNOTS = 30  # Speed threshold for anomalies\n",
    "trim_train_data = True\n",
    "remove_cog_sog_anomalies=True\n",
    "PORT_PROXIMITY_THRESHOLD= 50000 # Distance threshold for port proximity\n",
    "\n",
    "number_of_lagged_coordinates = 1\n",
    "list_of_features = ['lat_diff', 'lon_diff']\n",
    "moving_avg_features = []\n",
    "# New parameter for rolling window size\n",
    "rolling_window_size = 20  # You can adjust this as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "Data split done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'])\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "\n",
    "print('Splitting data...')\n",
    "if mode == 'predict':\n",
    "    # Extract the target variables for the training set\n",
    "    X_val = ais_test.copy()\n",
    "    X_train = ais_train.copy()\n",
    "    #remove all vessels that are not in the neighboorhood of the test set \n",
    "    \n",
    "else:\n",
    "    # Sort the entire dataset by 'time' to ensure order\n",
    "    ais_sorted = ais_train.sort_values(by='time')\n",
    "\n",
    "    # Calculate the threshold date for the last number_of_days days\n",
    "    threshold_date = ais_sorted['time'].max() - pd.Timedelta(days=number_of_days)\n",
    "\n",
    "    # Identify validation set indices where 'time' is within the last number_pf_days_days days\n",
    "    val_indices = ais_sorted[ais_sorted['time'] > threshold_date].index\n",
    "\n",
    "    # Use these indices to create the validation set\n",
    "    X_val = ais_train.loc[val_indices].reset_index(drop=True)\n",
    "\n",
    "    # Drop these indices from ais_train to get the training set\n",
    "    X_train = ais_train.drop(val_indices).reset_index(drop=True)\n",
    "\n",
    "    # Calculate the minimum date in the validation set (i.e., the first validation day)\n",
    "    first_val_time = X_val['time'].min()\n",
    "\n",
    "    # Add a column to the validation set for days since the first validation date\n",
    "    X_val['days_since_last_train'] = (pd.to_datetime(X_val['time']) - pd.to_datetime(first_val_time)).dt.days + 1\n",
    "\n",
    "    # Cap the days_since_last_train at 5\n",
    "    X_val['days_since_last_train'] = X_val['days_since_last_train'].apply(lambda day: min(day, 5))\n",
    "\n",
    "    # Remove a random x% of rows for each day in the validation set\n",
    "    if 0 < remove_fraction < 1:\n",
    "        # Group by 'days_since_last_train' and sample a fraction of rows from each day\n",
    "        X_val = X_val.groupby('days_since_last_train', group_keys=False).apply(\n",
    "            lambda group: group.sample(frac=(1 - remove_fraction), random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "print('Data split done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training records: 1522065\n",
      "Interpolating records with speed_knots > 30...\n",
      "Interpolating out-of-range COG and SOG values...\n",
      "Anomalies interpolated. Remaining training records: 1522065\n"
     ]
    }
   ],
   "source": [
    "# Copy datasets\n",
    "ais_test = X_val.copy()\n",
    "ais_train = X_train.copy()\n",
    "\n",
    "# Ignore future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Ensure 'time' is in datetime format and sort data\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "ais_train.sort_values(['vesselId', 'time'], inplace=True)\n",
    "\n",
    "# Convert relevant columns to numeric types\n",
    "numeric_columns = ['latitude_vessel', 'longitude_vessel', 'cog', 'sog']\n",
    "ais_train[numeric_columns] = ais_train[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(f\"Initial training records: {len(ais_train)}\")\n",
    "\n",
    "# =========================\n",
    "# Anomaly Detection and Interpolation for Latitude/Longitude\n",
    "# =========================\n",
    "print(f\"Interpolating records with speed_knots > {MAX_REALISTIC_SPEED_KNOTS}...\")\n",
    "\n",
    "# Calculate shifted coordinates\n",
    "ais_train['prev_latitude'] = ais_train.groupby('vesselId')['latitude_vessel'].shift(1)\n",
    "ais_train['prev_longitude'] = ais_train.groupby('vesselId')['longitude_vessel'].shift(1)\n",
    "\n",
    "# Function to compute distance between two points\n",
    "def compute_distance(row):\n",
    "    if pd.isnull(row['prev_latitude']) or pd.isnull(row['prev_longitude']):\n",
    "        return np.nan\n",
    "    else:\n",
    "        point1 = (row['prev_latitude'], row['prev_longitude'])\n",
    "        point2 = (row['latitude_vessel'], row['longitude_vessel'])\n",
    "        return geodesic(point1, point2).meters\n",
    "\n",
    "# Calculate distance in meters between consecutive points\n",
    "ais_train['distance_meters'] = ais_train.apply(compute_distance, axis=1)\n",
    "\n",
    "# Calculate time difference in seconds within each 'vesselId'\n",
    "ais_train['time_diff_seconds'] = ais_train.groupby('vesselId')['time'].diff().dt.total_seconds()\n",
    "ais_train['time_diff_seconds'].replace(0, np.nan, inplace=True)  # Avoid division by zero\n",
    "\n",
    "# Calculate speed in meters per second and convert to knots\n",
    "ais_train['speed_m_s'] = ais_train['distance_meters'] / ais_train['time_diff_seconds']\n",
    "ais_train['speed_knots'] = ais_train['speed_m_s'] * 1.94384\n",
    "\n",
    "# Flag speed anomalies\n",
    "ais_train['speed_anomaly'] = ais_train['speed_knots'] > MAX_REALISTIC_SPEED_KNOTS\n",
    "\n",
    "# Set anomalies to NaN in 'latitude_vessel' and 'longitude_vessel'\n",
    "ais_train.loc[ais_train['speed_anomaly'], ['latitude_vessel', 'longitude_vessel']] = np.nan\n",
    "\n",
    "# =========================\n",
    "# Anomaly Detection and Interpolation for COG/SOG\n",
    "# =========================\n",
    "print(\"Interpolating out-of-range COG and SOG values...\")\n",
    "\n",
    "# Flag COG anomalies\n",
    "ais_train['cog_anomaly'] = (ais_train['cog'] >= 360) | (ais_train['cog'] < 0)\n",
    "ais_train['sog_anomaly'] = ais_train['sog'] > 30\n",
    "\n",
    "# Set out-of-range COG and SOG values to NaN for interpolation\n",
    "ais_train.loc[ais_train['cog_anomaly'], 'cog'] = np.nan\n",
    "ais_train.loc[ais_train['sog_anomaly'], 'sog'] = np.nan\n",
    "\n",
    "# =========================\n",
    "# Interpolation for All Features\n",
    "# =========================\n",
    "# Set 'time' as the index for interpolation\n",
    "ais_train.set_index('time', inplace=True)\n",
    "\n",
    "# Interpolate within each 'vesselId' group and fill missing values forward/backward\n",
    "ais_train[['latitude_vessel', 'longitude_vessel', 'cog', 'sog']] = ais_train.groupby('vesselId')[\n",
    "    ['latitude_vessel', 'longitude_vessel', 'cog', 'sog']\n",
    "].transform(lambda group: group.interpolate(method='time').ffill().bfill())\n",
    "\n",
    "# Reset the index after interpolation\n",
    "ais_train.reset_index(inplace=True)\n",
    "\n",
    "# Drop the anomaly flag columns as they're no longer needed\n",
    "ais_train.drop(columns=['speed_anomaly', 'cog_anomaly', 'sog_anomaly'], inplace=True)\n",
    "\n",
    "# Drop any remaining NaN values resulting from shifts or interpolation limits\n",
    "ais_train.dropna(subset=['latitude_vessel', 'longitude_vessel', 'cog', 'sog'], inplace=True)\n",
    "\n",
    "print(f\"Anomalies interpolated. Remaining training records: {len(ais_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1522065\n",
      "1522065\n"
     ]
    }
   ],
   "source": [
    "print(len(ais_train))\n",
    "#remove all entries that have cog>=360 or cog<0\n",
    "ais_train = ais_train[ais_train['cog'] < 360]\n",
    "ais_train = ais_train[ais_train['cog'] >= 0]\n",
    "print(len(ais_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'cog', 'sog', 'rot', 'heading', 'navstat', 'etaRaw',\n",
      "       'latitude_vessel', 'longitude_vessel', 'vesselId', 'portId',\n",
      "       'prev_latitude', 'prev_longitude', 'distance_meters', 'speed_m_s',\n",
      "       'speed_knots', 'shippingLineId', 'CEU', 'DWT', 'GT', 'NT', 'vesselType',\n",
      "       'breadth', 'depth', 'draft', 'enginePower', 'freshWater', 'fuel',\n",
      "       'homePort', 'length', 'maxHeight', 'maxSpeed', 'maxWidth',\n",
      "       'rampCapacity', 'yearBuilt', 'name', 'portLocation', 'longitude',\n",
      "       'latitude', 'UN_LOCODE', 'countryName', 'ISO',\n",
      "       'prev1_time_diff_seconds', 'prev2_time_diff_seconds', 'prev1_longitude',\n",
      "       'prev1_latitude', 'prev2_longitude', 'prev2_latitude',\n",
      "       'prev3_longitude', 'prev3_latitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#merge the datasets with vessels\n",
    "ais_train = pd.merge(ais_train, vessels, on='vesselId', how='left')\n",
    "ais_test = pd.merge(ais_test, vessels, on='vesselId', how='left')\n",
    "\n",
    "#merge ais_train with ports\n",
    "ais_train = pd.merge(ais_train, ports, on='portId', how='left', suffixes=('', '_port'))\n",
    "\n",
    "\n",
    "# Ensure the DataFrames are sorted by 'vesselId' and 'time'\n",
    "ais_train = ais_train.sort_values(['vesselId', 'time'])\n",
    "ais_test = ais_test.sort_values(['vesselId', 'time'])\n",
    "\n",
    "# 3. Calculate the time difference between consecutive rows within each 'vesselId'\n",
    "ais_train['time_diff_seconds'] = ais_train.groupby('vesselId')['time'].diff().dt.total_seconds()\n",
    "\n",
    "\n",
    "# 4. Create lagged time difference features\n",
    "for i in range(1, number_of_lagged_coordinates + 2):\n",
    "    if i == 1:\n",
    "        # For lag 1, use the current 'time_diff_seconds' without shifting\n",
    "        ais_train[f'prev{i}_time_diff_seconds'] = ais_train['time_diff_seconds']\n",
    "        ais_test[f'prev{i}_time_diff_seconds'] = np.nan\n",
    "    else:\n",
    "        # For lags >1, shift 'time_diff_seconds' by (i -1)\n",
    "        ais_train[f'prev{i}_time_diff_seconds'] = ais_train.groupby('vesselId')['time_diff_seconds'].shift(i - 1)\n",
    "        ais_test[f'prev{i}_time_diff_seconds'] = np.nan\n",
    "    \n",
    "    # Append the feature name to the list of features\n",
    "for i in range(1, number_of_lagged_coordinates + 1):\n",
    "    list_of_features.append(f'prev{i}_time_diff_seconds')\n",
    "\n",
    "\n",
    "# 5. (Optional) Drop the intermediate 'time_diff_seconds' column if not needed\n",
    "ais_train.drop(columns=['time_diff_seconds'], inplace=True)\n",
    "\n",
    "\n",
    "#generate number_of_lagged_coordinates lagged coordinates and corresponting time differences\n",
    "for i in range(1, number_of_lagged_coordinates + 3):\n",
    "    ais_train[f'prev{i}_longitude'] = ais_train.groupby('vesselId')['longitude_vessel'].shift(i)\n",
    "    ais_train[f'prev{i}_latitude'] = ais_train.groupby('vesselId')['latitude_vessel'].shift(i)\n",
    "    ais_test[f'prev{i}_longitude'] = np.nan\n",
    "    ais_test[f'prev{i}_latitude'] = np.nan\n",
    "for i in range (1, number_of_lagged_coordinates + 1):\n",
    "    list_of_features.extend([f'prev{i}_longitude', f'prev{i}_latitude'])\n",
    "    \n",
    "        \n",
    "if moving_avg_features:\n",
    "    for feature in moving_avg_features.copy():\n",
    "        if feature=='cog':\n",
    "            \n",
    "            ais_train['cog_rad'] = np.radians(ais_train['cog'])\n",
    "            \n",
    "            ais_train['cog_sin'] = np.sin(ais_train['cog_rad'])\n",
    "            ais_train['cog_cos'] = np.cos(ais_train['cog_rad'])\n",
    "            \n",
    "            \n",
    "            # Compute moving averages for sine and cosine components\n",
    "            ais_train['cog_sin_moving_avg'] = ais_train.groupby('vesselId')['cog_sin'].transform(\n",
    "                lambda x: x.rolling(window=rolling_window_size, min_periods=1).mean()\n",
    "            )\n",
    "            ais_train['cog_cos_moving_avg'] = ais_train.groupby('vesselId')['cog_cos'].transform(\n",
    "                lambda x: x.rolling(window=rolling_window_size, min_periods=1).mean()\n",
    "            )\n",
    "            \n",
    "            ais_train['cog_avg_rad'] = np.arctan2(ais_train['cog_sin_moving_avg'], ais_train['cog_cos_moving_avg'])\n",
    "            ais_train['cog_avg_deg'] = (np.degrees(ais_train['cog_avg_rad'])) % 360\n",
    "            \n",
    "            # Update feature lists\n",
    "            moving_avg_features.remove('cog')  # Remove the original 'cog'\n",
    "            moving_avg_features.extend(['cog_sin', 'cog_cos'])  # Add sine and cosine moving averages\n",
    "            list_of_features.extend(['cog_avg_deg'])  # Add the reconstructed averaged COG\n",
    "            ais_test['cog_avg_deg'] = np.nan\n",
    "            ais_test['cog_sin_moving_avg'] = np.nan\n",
    "            ais_test['cog_cos_moving_avg'] = np.nan\n",
    "            ais_test['cog_avg_rad'] = np.nan\n",
    "               \n",
    "        else:\n",
    "            ais_train[f'{feature}_moving_avg'] = ais_train.groupby('vesselId')[feature].transform(lambda x: x.rolling(rolling_window_size, min_periods=1).mean())\n",
    "            ais_test[f'{feature}_moving_avg'] = np.nan\n",
    "            list_of_features.append(f'{feature}_moving_avg')\n",
    "\n",
    "#drop rows with NaN values due to shift\n",
    "ais_train.dropna(subset=['prev1_longitude', 'prev1_latitude', 'prev2_longitude', 'prev2_latitude'], inplace=True)\n",
    "\n",
    "print(ais_train.columns)\n",
    "            \n",
    "for feature in list_of_features.copy():\n",
    "    if feature == 'weekday':      \n",
    "        ais_train['weekday'] = ais_train['time'].dt.weekday.astype(int)\n",
    "        ais_test['weekday'] = ais_test['time'].dt.weekday.astype(int)\n",
    "    elif feature == 'month':\n",
    "        ais_train['month'] = ais_train['time'].dt.month\n",
    "        ais_test['month'] = ais_test['time'].dt.month\n",
    "    elif feature == 'hour':\n",
    "        ais_train['hour'] = ais_train['time'].dt.hour\n",
    "        ais_test['hour'] = ais_test['time'].dt.hour\n",
    "    # seconds after 1970\n",
    "    elif feature == 'total_seconds':\n",
    "        # Calculate `total_seconds` in a consistent manner across training and test sets\n",
    "        reference_time = min(ais_train['time'].min(), ais_test['time'].min())\n",
    "        # Update `total_seconds` calculation\n",
    "        ais_train['total_seconds'] = (ais_train['time'] - reference_time).dt.total_seconds()\n",
    "        ais_test['total_seconds'] = (ais_test['time'] - reference_time).dt.total_seconds()\n",
    "    elif feature == 'day_of_year':\n",
    "        ais_train['day_of_year'] = ais_train['time'].dt.dayofyear\n",
    "        ais_test['day_of_year'] = ais_test['time'].dt.dayofyear\n",
    "    elif feature == 'lon_diff':\n",
    "        ais_train['lon_diff'] = ((ais_train['prev1_longitude'] - ais_train['prev2_longitude'])/ais_train['prev2_time_diff_seconds']*ais_train['prev1_time_diff_seconds'])\n",
    "        ais_test['lon_diff'] = np.nan\n",
    "    elif feature == 'lat_diff':\n",
    "        ais_train['lat_diff'] = (((ais_train['prev1_latitude'] - ais_train['prev2_latitude'])/ais_train['prev2_time_diff_seconds'])*ais_train['prev1_time_diff_seconds'])\n",
    "        ais_test['lat_diff'] = np.nan\n",
    "    elif feature == 'lon_diff2':\n",
    "        ais_train['lon_diff2'] = ((ais_train['prev2_longitude'] - ais_train['prev3_longitude'])/ais_train['prev2_time_diff_seconds']*(ais_train['prev1_time_diff_seconds']+ais_train['prev2_time_diff_seconds']))\n",
    "        ais_test['lon_diff2'] = np.nan\n",
    "    elif feature == 'lat_diff2':\n",
    "        ais_train['lat_diff2'] = ((ais_train['prev2_latitude'] - ais_train['prev3_latitude'])/ais_train['prev2_time_diff_seconds']*(ais_train['prev1_time_diff_seconds']+ais_train['prev2_time_diff_seconds']))\n",
    "        ais_test['lat_diff2'] = np.nan\n",
    "    \n",
    "    elif feature == 'distance':\n",
    "        ais_train['distance'] = ais_train.apply(\n",
    "        lambda x: geodesic(\n",
    "            (x['prev2_latitude'], x['prev2_longitude']), \n",
    "            (x['prev1_latitude'], x['prev1_longitude'])  # Corrected\n",
    "            ).meters, \n",
    "            axis=1)\n",
    "    elif feature == 'xcogxtime':\n",
    "        ais_train['xcogxtime'] = ais_train['xcog']*ais_train['prev1_time_diff_seconds']\n",
    "        ais_test['xcogxtime'] = np.nan\n",
    "    elif feature == 'ycogxtime':\n",
    "        ais_train['ycogxtime'] = ais_train['ycog']*ais_train['prev1_time_diff_seconds']\n",
    "        ais_test['ycogytime'] = np.nan\n",
    "    elif feature=='sogxtime':\n",
    "        ais_train['sogxtime'] = ais_train['sog']*ais_train['prev1_time_diff_seconds']\n",
    "        ais_test['sogxtime'] = np.nan\n",
    "    elif feature.startswith('lag') and ('lat_diff' in feature or 'lon_diff' in feature):\n",
    "        lag_num = int(feature.split('_')[0][3:])\n",
    "        diff_type = 'lat_diff' if 'lat_diff' in feature else 'lon_diff'\n",
    "        ais_train[feature] = ais_train.groupby('vesselId')[diff_type].shift(lag_num)\n",
    "        ais_test[feature] = ais_test.groupby('vesselId')[diff_type].shift(lag_num)\n",
    "    elif feature == 'port_latitude':\n",
    "        ais_train['port_latitude'] = ais_train['latitude']\n",
    "        ais_test['port_latitude'] = np.nan\n",
    "    elif feature == 'port_longitude':\n",
    "        ais_train['port_longitude'] = ais_train['longitude']\n",
    "        ais_test['port_longitude'] = np.nan\n",
    "    elif feature == 'length':\n",
    "        ais_train['length'] = ais_train['length'].astype(int)\n",
    "        ais_test['length'] = ais_test['length'].astype(int)\n",
    "    \n",
    "if 'port_latitude' in list_of_features and 'port_longitude' in list_of_features:\n",
    "    # Calculate the distance to the port\n",
    "    ais_train['distance_to_port'] = ais_train.apply(\n",
    "        lambda x: np.nan if pd.isna(x['port_latitude']) else geodesic(\n",
    "            (x['prev1_latitude'], x['prev1_longitude']),\n",
    "            (x['port_latitude'], x['port_longitude'])\n",
    "        ).meters,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Set 'port_latitude' and 'port_longitude' to NaN if 'distance_to_port' is NaN\n",
    "    ais_train.loc[ais_train['distance_to_port'].isna(), ['port_latitude', 'port_longitude']] = -12345\n",
    "\n",
    "    # Set 'port_latitude' and 'port_longitude' to NaN if 'distance_to_port' exceeds the threshold\n",
    "    ais_train.loc[ais_train['distance_to_port'] > PORT_PROXIMITY_THRESHOLD, ['port_latitude', 'port_longitude']] = -12345\n",
    "\n",
    "\n",
    "#drop rows with NaN values due to shift\n",
    "ais_train.dropna(subset=list_of_features, inplace=True)\n",
    "        \n",
    "\n",
    "#sort by vesselId and time\n",
    "ais_train.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "ais_test.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if trim_train_data:\n",
    "    # Find the median number of rows per vessel in the training set\n",
    "    median_rows_per_vessel = ais_train.groupby('vesselId').size().median()\n",
    "    print(median_rows_per_vessel)\n",
    "\n",
    "    # Calculate the number of rows per vessel in the training set\n",
    "    rows_per_vessel = ais_train.groupby('vesselId').size()\n",
    "\n",
    "    # Find vessels with fewer than the median number of rows\n",
    "    vessels_with_few_rows = set(rows_per_vessel[rows_per_vessel < 100].index)\n",
    "\n",
    "    # Find vessels that are in the training set but not in the test set\n",
    "    vessels_not_in_test = set(ais_train['vesselId']) - set(ais_test['vesselId'])\n",
    "\n",
    "    # Combine conditions: vessels with few rows AND not in test set\n",
    "    vessels_to_remove = vessels_with_few_rows & vessels_not_in_test\n",
    "\n",
    "    # Remove rows corresponding to these vessels from the training set\n",
    "    ais_train = ais_train[~ais_train['vesselId'].isin(vessels_to_remove)]\n",
    "\n",
    "    #print number of negative time differences\n",
    "    print(ais_train[ais_train['prev1_time_diff_seconds'] < 0].shape[0])\n",
    "    \n",
    "if mode == 'validate':\n",
    "    #ais_test = ais_test[~ais_test['vesselId'].isin(vessels_to_remove)]\n",
    "    #if boat is not in train set but in test set, remove it from test set\n",
    "    ais_test = ais_test[ais_test['vesselId'].isin(ais_train['vesselId'])]\n",
    "    \n",
    "\n",
    "\n",
    "moving_avg_dicts = {}\n",
    "\n",
    "for feature in moving_avg_features:\n",
    "    moving_avg_dicts[f\"{feature}_moving_avg\"] = {}\n",
    "    \n",
    "    \n",
    "# Initialize with training data\n",
    "for vessel_id, group in ais_train.groupby('vesselId'):\n",
    "    #sort by time\n",
    "    group.sort_values(by='time', inplace=True) \n",
    "    for feature in moving_avg_features:\n",
    "        moving_avg_dicts[f\"{feature}_moving_avg\"][vessel_id] = deque(group[f'{feature}_moving_avg'].tail(rolling_window_size), maxlen=rolling_window_size)\n",
    "        \n",
    "moving_avg_dicts_pure = moving_avg_dicts.copy()\n",
    "        \n",
    "def update_moving_averages(vessel_id, feature, new_value):\n",
    "    moving_avg_dicts[f\"{feature}_moving_avg\"][vessel_id].append(new_value)\n",
    "    return np.mean(moving_avg_dicts[f\"{feature}_moving_avg\"][vessel_id])   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat_diff</th>\n",
       "      <th>lon_diff</th>\n",
       "      <th>prev1_time_diff_seconds</th>\n",
       "      <th>prev1_longitude</th>\n",
       "      <th>prev1_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078877</td>\n",
       "      <td>-0.100401</td>\n",
       "      <td>1583.0</td>\n",
       "      <td>77.49505</td>\n",
       "      <td>7.57302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.062838</td>\n",
       "      <td>-0.081995</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>77.39404</td>\n",
       "      <td>7.65043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061059</td>\n",
       "      <td>-0.078479</td>\n",
       "      <td>1259.0</td>\n",
       "      <td>77.31394</td>\n",
       "      <td>7.71275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.042338</td>\n",
       "      <td>-0.055885</td>\n",
       "      <td>901.0</td>\n",
       "      <td>77.23585</td>\n",
       "      <td>7.77191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.055026</td>\n",
       "      <td>-0.073090</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>77.18147</td>\n",
       "      <td>7.81285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lat_diff  lon_diff  prev1_time_diff_seconds  prev1_longitude  \\\n",
       "2  0.078877 -0.100401                   1583.0         77.49505   \n",
       "3  0.062838 -0.081995                   1285.0         77.39404   \n",
       "4  0.061059 -0.078479                   1259.0         77.31394   \n",
       "5  0.042338 -0.055885                    901.0         77.23585   \n",
       "6  0.055026 -0.073090                   1211.0         77.18147   \n",
       "\n",
       "   prev1_latitude  \n",
       "2         7.57302  \n",
       "3         7.65043  \n",
       "4         7.71275  \n",
       "5         7.77191  \n",
       "6         7.81285  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort and group by vesselId\n",
    "ais_train.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "\n",
    "X_for_training = ais_train[list_of_features]\n",
    "\n",
    "# Extract the target variables for the training set\n",
    "y_train_long = ais_train['longitude_vessel']\n",
    "y_train_lat = ais_train['latitude_vessel']\n",
    "\n",
    "X_for_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "Longitude model trained\n",
      "lat_diff: 0.0002100876049812494\n",
      "lon_diff: 0.001469552008038237\n",
      "prev1_time_diff_seconds: 0.003312593047416647\n",
      "prev1_longitude: 0.9947905400141245\n",
      "prev1_latitude: 0.00021722732543948446\n",
      "Latitude model trained\n",
      "All models trained.\n",
      "lat_diff: 0.0014628890215073404\n",
      "lon_diff: 0.0004925697711957313\n",
      "prev1_time_diff_seconds: 0.0019438170544288313\n",
      "prev1_longitude: 0.00036385319094865156\n",
      "prev1_latitude: 0.9957368709619194\n"
     ]
    }
   ],
   "source": [
    "print('Training models...')\n",
    "\n",
    "# Train model for longitude\n",
    "model_long = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1, random_state=42, verbose=0)\n",
    "model_long.fit(X_for_training, y_train_long)\n",
    "\n",
    "print('Longitude model trained')\n",
    "\n",
    "importances_long = model_long.feature_importances_\n",
    "for i in range(len(list_of_features)):\n",
    "    print(f\"{X_for_training.columns[i]}: {importances_long[i]}\")\n",
    "\n",
    "# Train model for latitude\n",
    "model_lat = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1, random_state=42, verbose=0)\n",
    "model_lat.fit(X_for_training, y_train_lat)\n",
    "\n",
    "print('Latitude model trained')\n",
    "\n",
    "print('All models trained.')\n",
    "\n",
    "#print feature importance\n",
    "\n",
    "importances_lat = model_lat.feature_importances_\n",
    "\n",
    "    \n",
    "for i in range(len(list_of_features)):\n",
    "    print(f\"{X_for_training.columns[i]}: {importances_lat[i]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0% complete.\n",
      "Processing: 1% complete.\n",
      "Processing: 2% complete.\n",
      "Processing: 3% complete.\n",
      "Processing: 4% complete.\n",
      "Processing: 5% complete.\n",
      "Processing: 6% complete.\n",
      "Processing: 7% complete.\n",
      "Processing: 8% complete.\n",
      "Processing: 9% complete.\n",
      "Processing: 10% complete.\n",
      "Processing: 11% complete.\n",
      "Processing: 12% complete.\n",
      "Processing: 13% complete.\n",
      "Processing: 14% complete.\n",
      "Processing: 15% complete.\n",
      "Processing: 16% complete.\n",
      "Processing: 17% complete.\n",
      "Processing: 18% complete.\n",
      "Processing: 19% complete.\n",
      "Processing: 20% complete.\n",
      "Processing: 21% complete.\n",
      "Processing: 22% complete.\n",
      "Processing: 23% complete.\n",
      "Processing: 24% complete.\n",
      "Processing: 25% complete.\n",
      "Processing: 26% complete.\n",
      "Processing: 27% complete.\n",
      "Processing: 28% complete.\n",
      "Processing: 29% complete.\n",
      "Processing: 30% complete.\n",
      "Processing: 31% complete.\n",
      "Processing: 32% complete.\n",
      "Processing: 33% complete.\n",
      "Processing: 34% complete.\n",
      "Processing: 35% complete.\n",
      "Processing: 36% complete.\n",
      "Processing: 37% complete.\n",
      "Processing: 38% complete.\n",
      "Processing: 39% complete.\n",
      "Processing: 40% complete.\n",
      "Processing: 41% complete.\n",
      "Processing: 42% complete.\n",
      "Processing: 43% complete.\n",
      "Processing: 44% complete.\n",
      "Processing: 45% complete.\n",
      "Processing: 46% complete.\n",
      "Processing: 47% complete.\n",
      "Processing: 48% complete.\n",
      "Processing: 49% complete.\n",
      "Processing: 50% complete.\n",
      "Processing: 51% complete.\n",
      "Processing: 52% complete.\n",
      "Processing: 53% complete.\n",
      "Processing: 54% complete.\n",
      "Processing: 55% complete.\n",
      "Processing: 56% complete.\n",
      "Processing: 57% complete.\n",
      "Processing: 58% complete.\n",
      "Processing: 59% complete.\n",
      "Processing: 60% complete.\n",
      "Processing: 61% complete.\n",
      "Processing: 62% complete.\n",
      "Processing: 63% complete.\n",
      "Processing: 64% complete.\n",
      "Processing: 65% complete.\n",
      "Processing: 66% complete.\n",
      "Processing: 67% complete.\n",
      "Processing: 68% complete.\n",
      "Processing: 69% complete.\n",
      "Processing: 70% complete.\n",
      "Processing: 71% complete.\n",
      "Processing: 72% complete.\n",
      "Processing: 73% complete.\n",
      "Processing: 74% complete.\n",
      "Processing: 75% complete.\n",
      "Processing: 76% complete.\n",
      "Processing: 77% complete.\n",
      "Processing: 78% complete.\n",
      "Processing: 79% complete.\n",
      "Processing: 80% complete.\n",
      "Processing: 81% complete.\n",
      "Processing: 82% complete.\n",
      "Processing: 83% complete.\n",
      "Processing: 84% complete.\n",
      "Processing: 85% complete.\n",
      "Processing: 86% complete.\n",
      "Processing: 87% complete.\n",
      "Processing: 88% complete.\n",
      "Processing: 89% complete.\n",
      "Processing: 90% complete.\n",
      "Processing: 91% complete.\n",
      "Processing: 92% complete.\n",
      "Processing: 93% complete.\n",
      "Processing: 94% complete.\n",
      "Processing: 95% complete.\n",
      "Processing: 96% complete.\n",
      "Processing: 97% complete.\n",
      "Processing: 98% complete.\n",
      "Processing: 99% complete.\n",
      "Processing: 100% complete.\n"
     ]
    }
   ],
   "source": [
    "#Make copy of moving_avg_dict\n",
    "moving_avg_dicts = moving_avg_dicts_pure.copy()\n",
    "\n",
    "# Initialize the prediction set from ais_test\n",
    "prediction_data = ais_test.copy()\n",
    "prediction_data['predicted_longitude'] = np.nan\n",
    "prediction_data['predicted_latitude'] = np.nan\n",
    "\n",
    "# Set up groups for efficient lookup by vessel\n",
    "train_set_sorted = ais_train.sort_values(by=['vesselId', 'time'])\n",
    "ais_train_groups = train_set_sorted.groupby('vesselId')\n",
    "\n",
    "# Initialize counters for progress tracking\n",
    "total_rows = len(prediction_data)\n",
    "progress_interval = max(total_rows // 100, 1)  # Calculate for 1% intervals, minimum 1\n",
    "processed_rows = 0  # Initialize counter for processed rows\n",
    "\n",
    "# Set model to use a single thread for faster individual row processing\n",
    "model_long.n_jobs = 1\n",
    "model_lat.n_jobs = 1\n",
    "\n",
    "# Loop through each vessel to perform sequential predictions\n",
    "for vessel_id, group in prediction_data.groupby('vesselId'):\n",
    "    # Get past data for this vessel\n",
    "    past_data = ais_train_groups.get_group(vessel_id)\n",
    "    # Track last known position within each vessel group\n",
    "    last_known_data = past_data.iloc[-1].copy()\n",
    "\n",
    "    last_known_data['predicted_longitude'] = last_known_data['longitude_vessel']\n",
    "    last_known_data['predicted_latitude'] = last_known_data['latitude_vessel']\n",
    "    \n",
    "\n",
    "    # Iterate over each row for this vessel in prediction_data\n",
    "    for idx, (index, row) in enumerate(group.iterrows()):\n",
    "        dict_avgs = {}\n",
    "        \n",
    "        \n",
    "        if 'cog_avg_deg' in list_of_features:\n",
    "            #calculate cog based on earlier coordinates in degrees\n",
    "            cog = np.degrees(np.arctan2(\n",
    "                last_known_data['predicted_longitude'] - last_known_data['prev1_longitude'], \n",
    "                last_known_data['predicted_latitude'] - last_known_data['prev1_latitude']\n",
    "            )) % 360\n",
    "            \n",
    "            \n",
    "            cog_conservative = 0.90 * cog + 0.1 * last_known_data['cog_avg_deg']\n",
    "            \n",
    "            # Decompose the updated COG into sine and cosine\n",
    "            cog_conservative_rad = np.radians(cog_conservative)\n",
    "            cog_sin_new = np.sin(cog_conservative_rad)\n",
    "            cog_cos_new = np.cos(cog_conservative_rad)\n",
    "            \n",
    "            # Update moving averages using deque or other methods\n",
    "            dict_avgs['cog_sin'] = update_moving_averages(vessel_id, 'cog_sin', cog_sin_new)\n",
    "            dict_avgs['cog_cos'] = update_moving_averages(vessel_id, 'cog_cos', cog_cos_new)\n",
    "\n",
    "            # Reconstruct the averaged COG\n",
    "            cog_avg_rad = np.arctan2(dict_avgs['cog_sin'], dict_avgs['cog_cos'])\n",
    "            cog_avg_deg = (np.degrees(cog_avg_rad)) % 360\n",
    "            \n",
    "            # Assign the averaged COG to the current prediction\n",
    "            prediction_data.at[index, 'cog_avg_deg'] = cog_avg_deg\n",
    "            \n",
    "        #calculate sog based on earlier coordinates\n",
    "            sog = (geodesic(\n",
    "                (last_known_data['predicted_latitude'], last_known_data['predicted_longitude']), \n",
    "                (last_known_data['prev1_latitude'], last_known_data['prev1_longitude'])\n",
    "            ).meters / last_known_data['prev1_time_diff_seconds']) / 0.51444  \n",
    "            sog_conservative = 0.90 * sog + 0.1 * last_known_data['sog_moving_avg']\n",
    "        #calculate xcog based on earlier coordinates\n",
    "        \n",
    "        \"\"\"\n",
    "        xcog = (last_known_data['predicted_longitude'] - last_known_data['prev1_longitude'])/last_known_data['prev1_time_diff_seconds']\n",
    "        xcog_conservative = 0.5 * xcog + 0.5 * last_known_data['xcog_moving_avg']\n",
    "        ycog = (last_known_data['predicted_latitude'] - last_known_data['prev1_latitude'])/last_known_data['prev1_time_diff_seconds']\n",
    "        ycog_conservative = 0.5 * ycog + 0.5 * last_known_data['ycog_moving_avg']\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for feature in moving_avg_features:\n",
    "            \n",
    "            if feature == 'latitude_vessel' :\n",
    "                dict_avgs[feature] = update_moving_averages( vessel_id, feature, last_known_data['predicted_latitude'])\n",
    "                \n",
    "            elif feature == 'longitude_vessel':\n",
    "                dict_avgs[feature] = update_moving_averages( vessel_id, feature, last_known_data['predicted_longitude'])\n",
    "                \n",
    "            elif feature == 'sog':\n",
    "                dict_avgs[feature] = update_moving_averages( vessel_id, feature, sog_conservative)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            elif feature=='xcog':    \n",
    "                dict_avgs[feature] = update_moving_averages( vessel_id, feature, xcog_conservative)\n",
    "                \n",
    "            elif feature=='ycog':\n",
    "                dict_avgs[feature] = update_moving_averages( vessel_id, feature, ycog_conservative)\n",
    "            \"\"\"\n",
    "            \n",
    "        # Assign moving averages to the current row\n",
    "        for feature in moving_avg_features:\n",
    "            prediction_data.at[index, f'{feature}_moving_avg'] = dict_avgs[feature]\n",
    "        \n",
    "        # Update lagged time difference dynamically\n",
    "        time_diff = (row['time'] - last_known_data['time']).total_seconds()\n",
    "        \n",
    "        if 'port_latitude' in list_of_features and 'port_longitude' in list_of_features:\n",
    "            #if port latitude is np.nan, set it to -12345\n",
    "            if pd.isna(last_known_data['port_latitude']):\n",
    "                last_known_data['port_latitude'] = -12345\n",
    "                last_known_data['port_longitude'] = -12345\n",
    "            \n",
    "            #check if port_lat is -12345\n",
    "            if last_known_data['port_latitude'] != -12345:\n",
    "                #calculate distance from predicted coordinates to port\n",
    "                distance_to_port = geodesic((last_known_data['predicted_latitude'], last_known_data['predicted_longitude']), (last_known_data['port_latitude'], last_known_data['port_longitude'])).meters\n",
    "                if distance_to_port < PORT_PROXIMITY_THRESHOLD:\n",
    "                    prediction_data.at[index, 'port_latitude'] = last_known_data['port_latitude']\n",
    "                    prediction_data.at[index, 'port_longitude'] = last_known_data['port_longitude']\n",
    "                else:\n",
    "                    prediction_data.at[index, 'port_latitude'] = -12345\n",
    "                    prediction_data.at[index, 'port_longitude'] = -12345\n",
    "            else:\n",
    "                prediction_data.at[index, 'port_latitude'] = -12345\n",
    "                prediction_data.at[index, 'port_longitude'] = -12345\n",
    "        \n",
    "        prediction_data.at[index, 'prev1_time_diff_seconds'] = time_diff\n",
    "        prediction_data.at[index, 'prev2_time_diff_seconds'] = last_known_data['prev1_time_diff_seconds']\n",
    "        \n",
    "        # Dynamically update lagged coordinates\n",
    "        prediction_data.at[index, 'prev1_longitude'] = last_known_data['predicted_longitude']\n",
    "        prediction_data.at[index, 'prev1_latitude'] = last_known_data['predicted_latitude']\n",
    "        prediction_data.at[index, 'prev2_latitude'] = last_known_data['prev1_latitude']\n",
    "        prediction_data.at[index, 'prev2_longitude'] = last_known_data['prev1_longitude']\n",
    "        \n",
    "        if 'lat_diff' in list_of_features:\n",
    "            prediction_data.at[index, 'lat_diff'] = (((last_known_data['predicted_latitude'] - last_known_data['prev1_latitude'])/last_known_data['prev1_time_diff_seconds'])*time_diff)\n",
    "            prediction_data.at[index, 'lon_diff'] = (((last_known_data['predicted_longitude'] - last_known_data['prev1_longitude'])/last_known_data['prev1_time_diff_seconds'])*time_diff)\n",
    "        if 'lat_diff2' in list_of_features:\n",
    "            prediction_data.at[index, 'lat_diff2'] = (((last_known_data['prev1_latitude'] - last_known_data['prev2_latitude'])/last_known_data['prev2_time_diff_seconds'])*(last_known_data['prev1_time_diff_seconds']+last_known_data['prev2_time_diff_seconds']))\n",
    "            prediction_data.at[index, 'lon_diff2'] = (((last_known_data['prev1_longitude'] - last_known_data['prev2_longitude'])/last_known_data['prev2_time_diff_seconds'])*(last_known_data['prev1_time_diff_seconds']+last_known_data['prev2_time_diff_seconds']))\n",
    "            \n",
    "        \n",
    "        if 'distance' in list_of_features:\n",
    "            prediction_data.at[index, 'distance'] = geodesic((last_known_data['prev1_latitude'], last_known_data['prev1_longitude']), (last_known_data['predicted_latitude'], last_known_data['predicted_longitude'])).meters\n",
    "        \n",
    "        if 'xcogxtime' in list_of_features:\n",
    "            prediction_data.at[index, 'xcogxtime'] = prediction_data.at[index, 'xcog_moving_avg']*time_diff\n",
    "            \n",
    "        if 'ycogxtime' in list_of_features:\n",
    "            prediction_data.at[index, 'ycogxtime'] = prediction_data.at[index, 'ycog_moving_avg']*time_diff\n",
    "        \n",
    "        if 'sogxtime' in list_of_features:\n",
    "            prediction_data.at[index, 'sogxtime'] = prediction_data.at[index, 'sog_moving_avg']*time_diff\n",
    "        \n",
    "    # Update multiple lag features\n",
    "        for feature in list_of_features:\n",
    "            if feature.startswith('lag') and ('lat_diff' in feature or 'lon_diff' in feature):\n",
    "                lag_num = int(feature.split('_')[0][3:])\n",
    "                diff_type = 'lat_diff' if 'lat_diff' in feature else 'lon_diff'\n",
    "                if lag_num == 1:\n",
    "                    prediction_data.at[index, feature] = last_known_data[diff_type]\n",
    "                else:\n",
    "                    prev_lag_feature = f'lag{lag_num - 1}_{diff_type}'\n",
    "                    prediction_data.at[index, feature] = last_known_data.get(prev_lag_feature, np.nan)\n",
    "\n",
    "        \n",
    "        if number_of_lagged_coordinates > 1:\n",
    "            # Update higher-order lag features based on latest predictions\n",
    "            for i in range(2, number_of_lagged_coordinates + 1):  \n",
    "                # Shift previous predictions back for each lag\n",
    "                prediction_data.at[index, f'prev{i}_longitude'] = last_known_data.get(f'prev{i-1}_longitude', np.nan)\n",
    "                prediction_data.at[index, f'prev{i}_latitude'] = last_known_data.get(f'prev{i-1}_latitude', np.nan)\n",
    "                prediction_data.at[index, f'prev{i}_time_diff_seconds'] = last_known_data.get(f'prev{i-1}_time_diff_seconds', np.nan)\n",
    "\n",
    "        \n",
    "        features = prediction_data.loc[index, list_of_features].to_frame().T\n",
    "\n",
    "                    \n",
    "        # Make predictions for the current row\n",
    "        pred_long = model_long.predict(features)[0]\n",
    "        pred_lat = model_lat.predict(features)[0]\n",
    "        \n",
    "        #pred_lat, pred_long = adjust_to_sea(pred_lat, pred_long)\n",
    "        \n",
    "        # Store predictions\n",
    "        prediction_data.at[index, 'predicted_longitude'] = pred_long\n",
    "        prediction_data.at[index, 'predicted_latitude'] = pred_lat\n",
    "        \n",
    "        # Update last known data with current prediction\n",
    "        last_known_data = prediction_data.loc[index].copy()\n",
    "        last_known_data['time'] = row['time']\n",
    "        \n",
    "        # Handling lag_lat_diff and lag_lon_diff\n",
    "        for feature in list_of_features:\n",
    "            if feature.startswith('lag') and 'lat_diff' in feature:\n",
    "                prediction_data.at[index, feature] = last_known_data['lat_diff']\n",
    "            if feature.startswith('lag') and 'lon_diff' in feature:\n",
    "                prediction_data.at[index, feature] = last_known_data['lon_diff']\n",
    "        \n",
    "        \n",
    "        # Increment the counter for processed rows and print progress\n",
    "        processed_rows += 1\n",
    "        if processed_rows % progress_interval == 0:\n",
    "            percent_complete = int((processed_rows / total_rows) * 100)\n",
    "            print(f\"Processing: {percent_complete}% complete.\")\n",
    "                \n",
    "print(\"Processing: 100% complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed predictions saved to Detailed_Predictions.csv.\n",
      "Submission predictions saved to Submission.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#outputs\n",
    "if mode=='predict':\n",
    "    \n",
    "    prediction_data['longitude_predicted'] = prediction_data['predicted_longitude']\n",
    "    prediction_data['latitude_predicted'] = prediction_data['predicted_latitude']\n",
    "    detailed_predictions = prediction_data\n",
    "    \n",
    "    detailed_predictions.to_csv('Detailed_Predictions.csv', index=False)\n",
    "    print(\"Detailed predictions saved to Detailed_Predictions.csv.\")\n",
    "\n",
    "    submission_predictions = prediction_data[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "    submission_predictions.to_csv('Submission.csv', index=False)\n",
    "    print(\"Submission predictions saved to Submission.csv.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tdt4173-main-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
